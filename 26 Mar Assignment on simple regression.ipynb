{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ee94c-497e-4e13-89e0-b2704fd44bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-1:\n",
    "    Simple linear regression is a statistical method that models the relationship between two variables, typically denoted as \n",
    "\n",
    " (independent variable) and \n",
    "\n",
    "(dependent variable), using a linear equation. The equation takes the form:\n",
    "    Let's say you want to predict a person's weight (\n",
    "\n",
    "based on their height (\n",
    "You collect data and find a linear relationship between height and weight.\n",
    "    The simple linear regression equation might look like:\n",
    "\n",
    "Îµ accounts for factors not considered in the model.\n",
    "    \n",
    "    Multiple linear regression extends the concept of simple linear regression to model the relationship between a dependent variable \n",
    "\n",
    "Y and multiple independent variables\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9cc29-a3ba-49c2-aa12-8512858fde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-2:\n",
    "    Linear regression relies on several assumptions to ensure that the model provides valid and reliable results. These assumptions include:\n",
    "\n",
    "1. Linearity:\n",
    "   - Assumption: The relationship between the independent and dependent variables is linear.\n",
    "   - Checking: Scatter plots of the dependent variable against each independent variable and the residuals should exhibit a linear pattern.\n",
    "\n",
    "2. Independence of Residuals:\n",
    "   - Assumption: Residuals (the differences between observed and predicted values) are independent of each other.\n",
    "   - Checking: Plotting residuals against predicted values or time can help identify patterns or trends.\n",
    "\n",
    "3. Homoscedasticity (Constant Variance of Residuals):\n",
    "   - Assumption: The variance of residuals is constant across all levels of the independent variables.\n",
    "   - Checking: A scatter plot of residuals against predicted values should show a constant spread.\n",
    "\n",
    "4. Normality of Residuals:\n",
    "   - Assumption: Residuals are normally distributed.\n",
    "   - Checking: A histogram or a Q-Q plot of residuals can be examined for normality. Statistical tests like the Shapiro-Wilk test can also be used.\n",
    "\n",
    "5. No Perfect Multicollinearity:\n",
    "   - Assumption: Independent variables are not perfectly correlated with each other.\n",
    "   - Checking: Calculate the variance inflation factor (VIF) for each independent variable. High VIF values (usually above 10) may indicate multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2deba-0fe6-498c-8084-e31eff1dd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-3:n a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "\n",
    "Intercept\n",
    "\n",
    "The intercept represents the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "It may or may not have a meaningful interpretation depending on the context. For example, in some cases, a zero value for an independent variable might be nonsensical.\n",
    "In practical terms, the intercept is the starting point of the regression line.\n",
    "\n",
    "\n",
    "Slope \n",
    "The slope represents the change in the dependent variable for a one-unit change in the corresponding independent variable, assuming all other variables are held constant.\n",
    "It quantifies the strength and direction of the relationship between the independent and dependent variables.\n",
    "A positive slope indicates a positive relationship, while a negative slope indicates a negative relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fc9d0-56c7-49a3-988d-bbaea97877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-4:**Gradient Descent:**\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in the context of machine learning models. The goal of training a machine learning model is to find the optimal parameters (weights and biases) that minimize the difference between the predicted values and the actual values. The cost function is a measure of this difference.\n",
    "\n",
    "The basic idea behind gradient descent is to iteratively move towards the minimum of the cost function by adjusting the model parameters. It's akin to finding the lowest point in a valley by moving in the steepest direction downhill.\n",
    "\n",
    "Here are the key steps involved in the gradient descent algorithm:\n",
    "\n",
    "1. **Initialize Parameters:**\n",
    "   - Start with initial values for the model parameters (weights and biases).\n",
    "\n",
    "2. **Compute the Gradient:**\n",
    "   - Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. The gradient indicates the direction of the steepest ascent.\n",
    "\n",
    "3. **Update Parameters:**\n",
    "   - Adjust the parameters in the opposite direction of the gradient to move towards the minimum. This is done by multiplying the gradient by a learning rate and subtracting the result from the current parameter values.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Repeat steps 2 and 3 until convergence or a predefined number of iterations.\n",
    "\n",
    "**Learning Rate:**\n",
    "The learning rate is a hyperparameter that determines the size of the steps taken during each iteration of gradient descent. If the learning rate is too small, the algorithm may take a long time to converge, while if it's too large, it might overshoot the minimum and fail to converge.\n",
    "\n",
    "**Types of Gradient Descent:**\n",
    "1. **Batch Gradient Descent:**\n",
    "   - Computes the gradient of the cost function with respect to the parameters for the entire training dataset.\n",
    "   - Updates the parameters after processing the entire dataset.\n",
    "  \n",
    "2. **Stochastic Gradient Descent (SGD):**\n",
    "   - Computes the gradient and updates the parameters for each training example (or a small batch) individually.\n",
    "   - Converges faster but introduces more variability.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent:**\n",
    "   - Combines aspects of both batch and stochastic gradient descent by updating the parameters using a small batch of training examples.\n",
    "\n",
    "**How Gradient Descent is Used in Machine Learning:**\n",
    "\n",
    "Gradient descent is a fundamental optimization algorithm in machine learning used in various contexts, including:\n",
    "\n",
    "1. **Training Neural Networks:**\n",
    "   - Adjusting weights and biases in neural networks during the backpropagation process.\n",
    "\n",
    "2. **Linear Regression:**\n",
    "   - Optimizing the parameters (slope and intercept) to minimize the mean squared error.\n",
    "\n",
    "3. **Logistic Regression:**\n",
    "   - Minimizing the cost function to find the optimal parameters for binary classification.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - Optimizing the hyperplane parameters to maximize the margin between classes.\n",
    "\n",
    "In essence, gradient descent is a crucial tool for fine-tuning model parameters and improving the accuracy and performance of machine learning models during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3e90f-74f3-4391-8a5b-2220c276becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-5:\n",
    "    Multiple linear regression is commonly used \n",
    "    when there are multiple factors that may influence\n",
    "    the dependent variable. It allows for a more realistic\n",
    "    representation of complex relationships in various fields\n",
    "    such as economics, finance, biology, and social sciences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb2458-a7fa-423e-8cb7-858957a02d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-6:**Multicollinearity in Multiple Linear Regression:**\n",
    "\n",
    "Multicollinearity is a statistical phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated with each other. This high correlation can lead to issues in estimating the individual contributions of each variable to the dependent variable. In other words, it becomes challenging to isolate the unique effect of each independent variable because their effects are entangled.\n",
    "\n",
    "**Detecting Multicollinearity:**\n",
    "\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "1. **Correlation Matrix:**\n",
    "   - Examine the correlation matrix among independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF):**\n",
    "   - Calculate the VIF for each independent variable. The VIF measures how much the variance of an estimated regression coefficient increases if the predictors are correlated. High VIF values (typically above 10) suggest multicollinearity.\n",
    "\n",
    "3. **Tolerance:**\n",
    "   - Tolerance is the reciprocal of the VIF. Low tolerance values (close to 0) indicate high multicollinearity.\n",
    "\n",
    "4. **Eigenvalues:**\n",
    "   - Examine the eigenvalues of the correlation matrix. If there are one or more very small eigenvalues, multicollinearity may be present.\n",
    "\n",
    "**Addressing Multicollinearity:**\n",
    "\n",
    "1. **Feature Selection:**\n",
    "   - Remove one or more of the highly correlated variables. Choose the most relevant variables for your analysis based on domain knowledge or feature importance.\n",
    "\n",
    "2. **Combine Variables:**\n",
    "   - If possible, create new variables that are combinations of the correlated variables. For example, if height and weight are highly correlated, create a body mass index (BMI) variable.\n",
    "\n",
    "3. **Regularization:**\n",
    "   - Techniques like Ridge Regression or Lasso Regression can be employed. These methods introduce a penalty term to the regression equation, which can help in reducing the impact of multicollinearity.\n",
    "\n",
    "4. **Increase Sample Size:**\n",
    "   - Collect more data to reduce the impact of multicollinearity. With a larger sample size, estimates become more stable.\n",
    "\n",
    "5. **Principal Component Analysis (PCA):**\n",
    "   - Use PCA to transform the original correlated variables into a set of linearly uncorrelated variables (principal components).\n",
    "\n",
    "6. **Centering Variables:**\n",
    "   - Standardize or center the variables to minimize the impact of scale differences.\n",
    "\n",
    "**Implications of Multicollinearity:**\n",
    "\n",
    "1. **Unreliable Coefficient Estimates:**\n",
    "   - The coefficients of the correlated variables become unstable and may have large standard errors.\n",
    "\n",
    "2. **Inflated Standard Errors:**\n",
    "   - In the presence of multicollinearity, the standard errors of the coefficients tend to be inflated.\n",
    "\n",
    "3. **Difficulty in Interpretation:**\n",
    "   - It becomes challenging to interpret the individual impact of each variable on the dependent variable.\n",
    "\n",
    "Addressing multicollinearity is essential to ensure the validity and reliability of the multiple linear regression model. The specific approach depends on the context of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8795c-bb7b-473f-89d8-6c8d772bf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-7:Polynomial regression is a type of regression analysis \n",
    "where the relationship between the independent variable (\n",
    "\n",
    "X) and the dependent variable (\n",
    "\n",
    "Y) is modeled as an \n",
    "\n",
    "n-th degree polynomial. \n",
    "Differences from Linear Regression:\n",
    "\n",
    "Functional Form:\n",
    "\n",
    "Linear regression assumes a linear relationship between the independent and dependent variables, while polynomial regression allows for a nonlinear relationship by including terms like \n",
    "\n",
    " .\n",
    "Flexibility:\n",
    "\n",
    "Polynomial regression is more flexible than linear regression in capturing non-linear patterns in the data. It can model curves and bends in the relationship.\n",
    "Degree of the Polynomial:\n",
    "\n",
    "The degree of the polynomial (\n",
    "\n",
    "n) determines the complexity of the model. Higher-degree polynomials can capture more intricate patterns but may be prone to overfitting.\n",
    "Interpretability:\n",
    "\n",
    "Linear regression coefficients have straightforward interpretations, representing the change in the dependent variable for a one-unit change in the independent variable. In polynomial regression, the interpretation becomes more complex as it involves the effect of changes in the independent variable and its powers.\n",
    "Risk of Overfitting:\n",
    "\n",
    "As the degree of the polynomial increases, the model may become too complex and fit the training data too closely, leading to overfitting. Overfit models may perform poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7083a-98f1-44e0-8413-930a354cb78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
